Model: <class 'models.ppo.PPOAgent'>, Dir: iter1/
statemodel: <class 'utils.envs.WorldModel'>, num_envs: 16,

import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE

EPS_MIN = 0.2                 	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.997             	# The rate at which eps decays from EPS_MAX to EPS_MIN
BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 5					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.005				# The limit of the ratio of new action probabilities to old probabilities
DISCOUNT_RATE = 0.97			# The discount rate to use in the Bellman Equation
NUM_STEPS = 20					# The number of steps to collect experience in sequence for each GAE calculation
ADVANTAGE_DECAY = 0.99			# The discount factor for the cumulative GAE calculation

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = action_mu if not sample else dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu() + state
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			value = self.critic_local(state.to(self.device))
			return value

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=1, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) + critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages).mean() + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss)
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = PrioritizedReplayBuffer()
		self.ppo_epochs = PPO_EPOCHS
		self.ppo_batch = BATCH_SIZE

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		update_freq = int(self.update_freq * (1 - self.eps + EPS_MIN)**0.0)
		if len(self.buffer) >= update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(zip(states, actions, log_probs, targets, advantages))
			for _ in range(self.ppo_epochs):
				self.replay_buffer.reset_priorities()
				for _ in range(update_freq):
					(state, action, log_prob, target, advantage), indices, importances = self.replay_buffer.sample(self.ppo_batch, dtype=torch.stack)
					errors = self.network.optimize(state, action, log_prob, target, advantage, importances**(1-self.eps))
					self.replay_buffer.update_priorities(indices, errors)
		if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.97			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.0225               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import os
import gym
import torch
import argparse
import numpy as np
from collections import deque
from models.ppo import PPOAgent
from models.rand import RandomAgent
from models.ddpg import DDPGAgent, EPS_MIN
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, WorldModel, ImgStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--iternum", type=int, default=-1, choices=[-1,0,1], help="Whether to train using World Model to load (0 or 1) or raw images (-1)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddpg", "ppo"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--runs", type=int, default=1, help="Number of episodes to train the agent")
parser.add_argument("--trial", action="store_true", help="Whether to show a trial run training on the Pendulum-v0 environment")
args = parser.parse_args()

ENV_NAME = "CarRacing-v0"

class WorldACAgent(RandomAgent):
	def __init__(self, action_size, num_envs, acagent, statemodel=WorldModel, load="", gpu=True, train=True):
		super().__init__(action_size)
		self.world_model = statemodel(action_size, num_envs, load=load, gpu=gpu)
		self.acagent = acagent(self.world_model.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state, latent = self.world_model.get_state(state)
		env_action, action = self.acagent.get_env_action(env, state, eps, sample)
		self.world_model.step(latent, env_action)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.world_model.get_state(next_state)[0]
		self.acagent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.world_model.num_envs if num_envs is None else num_envs
		self.world_model.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		self.acagent.network.save_model(dirname, name)

	def load(self, dirname="pytorch", name="best"):
		self.world_model.load_model(dirname, name)
		self.acagent.network.load_model(dirname, name)
		return self

def run(model, statemodel, runs=1, load_dir="", ports=16):
	num_envs = len(ports) if type(ports) == list else min(ports, 16)
	logger = Logger(model, load_dir, statemodel=statemodel, num_envs=num_envs)
	envs = EnvManager(ENV_NAME, ports) if type(ports) == list else EnsembleEnv(ENV_NAME, ports)
	agent = WorldACAgent(envs.action_size, num_envs, model, statemodel, load=load_dir)
	total_rewards = []
	for ep in range(runs):
		states = envs.reset()
		agent.reset(num_envs)
		total_reward = 0
		for _ in range(envs.env.spec.max_episode_steps):
			env_actions, actions, states = agent.get_env_action(envs.env, states)
			next_states, rewards, dones, _ = envs.step(env_actions, render=(ep%runs==0))
			agent.train(states, actions, next_states, rewards, dones)
			total_reward += np.mean(rewards)
			states = next_states
		rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(10)]
		test_reward = np.mean(rollouts) - np.std(rollouts)
		total_rewards.append(test_reward)
		agent.save_model(load_dir, "checkpoint")
		if total_rewards[-1] >= max(total_rewards): agent.save_model(load_dir)
		logger.log(f"Ep: {ep}, Reward: {total_reward:.4f}, Test: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.acagent.eps:.3f})")
	envs.close()

def trial(model, steps=40000, ports=16):
	env_name = "Pendulum-v0"
	envs = EnvManager(ENV_NAME, ports) if type(ports) == list else EnsembleEnv(ENV_NAME, ports)
	agent = model(envs.state_size, envs.action_size, decay=0.99)
	env = gym.make(env_name)
	state = envs.reset()
	test_rewards = []
	for s in range(steps):
		env_action, action = agent.get_env_action(env, state)
		next_state, reward, done, _ = envs.step(env_action)
		agent.train(state, action, next_state, reward, done)
		state = next_state
		if s % env.spec.max_episode_steps == 0:
			test_reward = np.mean([rollout(env, agent) for _ in range(10)])
			test_rewards.append(test_reward)
			print(f"Ep: {s//env.spec.max_episode_steps}, Rewards: {test_reward}, Avg: {np.mean(test_rewards)}")
			if test_reward > -200: break
	env.close()
	envs.close()

if __name__ == "__main__":
	dirname = "pytorch" if args.iternum < 0 else f"iter{args.iternum}/"
	state = ImgStack if args.iternum < 0 else WorldModel
	model = PPOAgent if args.model == "ppo" else DDPGAgent
	if args.trial:
		trial(model, ports=args.workerports)
	elif args.selfport is not None:
		EnvWorker(args.selfport, ENV_NAME).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, state, args.runs, dirname, args.workerports)

Ep: 0, Reward: -44.2421, Test: -18.6440 [14.32], Avg: -32.9608 (0.997)
Ep: 1, Reward: -44.5726, Test: -20.7491 [11.34], Avg: -32.5271 (0.994)
Ep: 2, Reward: -42.5393, Test: -21.5308 [18.85], Avg: -35.1441 (0.991)
Ep: 3, Reward: -39.7363, Test: -25.7539 [13.49], Avg: -36.1696 (0.988)
Ep: 4, Reward: -36.3201, Test: -12.9371 [19.60], Avg: -35.4429 (0.985)
Ep: 5, Reward: -35.6390, Test: -16.1597 [18.36], Avg: -35.2896 (0.982)
Ep: 6, Reward: -30.4331, Test: -6.1798 [21.73], Avg: -34.2357 (0.979)
Ep: 7, Reward: -28.0103, Test: -26.7923 [8.89], Avg: -34.4162 (0.976)
Ep: 8, Reward: -24.1257, Test: -15.6605 [23.62], Avg: -34.9566 (0.973)
Ep: 9, Reward: -23.8360, Test: -13.7537 [27.35], Avg: -35.5715 (0.970)
Ep: 10, Reward: -24.6879, Test: -24.2533 [14.73], Avg: -35.8818 (0.967)
Ep: 11, Reward: -29.5163, Test: -24.0249 [15.24], Avg: -36.1635 (0.965)
Ep: 12, Reward: -20.5740, Test: -26.0437 [6.62], Avg: -35.8943 (0.962)
Ep: 13, Reward: -16.7825, Test: -24.7163 [9.04], Avg: -35.7415 (0.959)
Ep: 14, Reward: -21.1573, Test: -15.6534 [19.72], Avg: -35.7172 (0.956)
Ep: 15, Reward: -15.2951, Test: -23.1220 [13.71], Avg: -35.7871 (0.953)
Ep: 16, Reward: -22.1235, Test: -20.0006 [10.04], Avg: -35.4489 (0.950)
Ep: 17, Reward: -16.1343, Test: -20.6827 [13.47], Avg: -35.3767 (0.947)
Ep: 18, Reward: -24.7315, Test: -22.9849 [3.91], Avg: -34.9304 (0.945)
Ep: 19, Reward: -25.3719, Test: -38.8556 [28.73], Avg: -36.5630 (0.942)
Ep: 20, Reward: -25.1475, Test: -30.3578 [38.74], Avg: -38.1123 (0.939)
Ep: 21, Reward: -15.7936, Test: -32.1405 [57.78], Avg: -40.4674 (0.936)
Ep: 22, Reward: -37.5980, Test: 20.4590 [64.80], Avg: -40.6359 (0.933)
Ep: 23, Reward: -5.8705, Test: 44.1378 [34.52], Avg: -38.5421 (0.930)
Ep: 24, Reward: -9.3362, Test: -7.4859 [41.50], Avg: -38.9598 (0.928)
Ep: 25, Reward: 7.0418, Test: -52.0921 [19.17], Avg: -40.2023 (0.925)
Ep: 26, Reward: -20.3514, Test: -32.3264 [54.20], Avg: -41.9182 (0.922)
Ep: 27, Reward: 8.4130, Test: 0.4153 [33.02], Avg: -41.5854 (0.919)
Ep: 28, Reward: -0.6397, Test: -23.8530 [36.96], Avg: -42.2484 (0.917)
Ep: 29, Reward: 26.0288, Test: 0.2750 [33.34], Avg: -41.9422 (0.914)
Ep: 30, Reward: 5.6746, Test: -33.0859 [28.22], Avg: -42.5669 (0.911)
Ep: 31, Reward: 27.0814, Test: -30.2322 [29.01], Avg: -43.0881 (0.908)
Ep: 32, Reward: 22.2292, Test: 5.8707 [43.99], Avg: -42.9376 (0.906)
Ep: 33, Reward: 43.3239, Test: -1.7645 [49.44], Avg: -43.1806 (0.903)
Ep: 34, Reward: 21.7053, Test: -51.6290 [12.15], Avg: -43.7691 (0.900)
Ep: 35, Reward: -14.2464, Test: -34.7354 [30.22], Avg: -44.3576 (0.897)
Ep: 36, Reward: -16.7145, Test: -24.1575 [26.82], Avg: -44.5364 (0.895)
Ep: 37, Reward: -22.9908, Test: -31.1873 [22.21], Avg: -44.7695 (0.889)
Ep: 38, Reward: -25.5857, Test: -46.5527 [10.64], Avg: -45.0881 (0.887)
Ep: 39, Reward: -36.1149, Test: -32.0465 [17.36], Avg: -45.1959 (0.884)
Ep: 40, Reward: -27.3080, Test: -47.2826 [11.67], Avg: -45.5314 (0.881)
Ep: 41, Reward: -23.8284, Test: -39.8948 [8.96], Avg: -45.6106 (0.879)
Ep: 42, Reward: -18.9637, Test: -35.5551 [23.12], Avg: -45.9143 (0.876)
Ep: 43, Reward: -10.2864, Test: -28.9644 [36.01], Avg: -46.3474 (0.874)
Ep: 44, Reward: -3.0745, Test: -2.7156 [40.79], Avg: -46.2843 (0.871)
Ep: 45, Reward: -19.2142, Test: 87.6546 [98.38], Avg: -45.5113 (0.868)
Ep: 46, Reward: 100.4672, Test: 127.2904 [121.53], Avg: -44.4203 (0.866)
Ep: 47, Reward: 133.8782, Test: 118.7390 [102.48], Avg: -43.1562 (0.863)
Ep: 48, Reward: 187.8562, Test: 137.2724 [110.75], Avg: -41.7342 (0.861)
Ep: 49, Reward: 149.0722, Test: 117.2985 [105.50], Avg: -40.6635 (0.858)
Ep: 50, Reward: 196.7401, Test: 171.5596 [128.09], Avg: -39.0139 (0.855)
Ep: 51, Reward: 327.0013, Test: 174.8842 [123.19], Avg: -37.2696 (0.853)
Ep: 52, Reward: 220.0148, Test: 161.6147 [146.82], Avg: -36.2872 (0.850)
Ep: 53, Reward: 262.7970, Test: 144.7200 [105.56], Avg: -34.8899 (0.848)
Ep: 54, Reward: 257.1162, Test: 129.3300 [104.88], Avg: -33.8109 (0.845)
Ep: 55, Reward: 166.5119, Test: 134.3515 [99.14], Avg: -32.5785 (0.843)
Ep: 56, Reward: 162.1453, Test: 86.0389 [98.35], Avg: -32.2229 (0.840)
Ep: 57, Reward: 158.4267, Test: 92.4635 [80.87], Avg: -31.4676 (0.838)
Ep: 58, Reward: 157.1844, Test: 154.8934 [119.97], Avg: -30.3424 (0.835)
Ep: 59, Reward: 129.1898, Test: 71.4305 [102.02], Avg: -30.3465 (0.833)
Ep: 60, Reward: 135.1705, Test: 155.4013 [113.56], Avg: -29.1632 (0.830)
Ep: 61, Reward: 170.8534, Test: 88.6657 [102.20], Avg: -28.9111 (0.828)
Ep: 62, Reward: 141.6496, Test: 151.0557 [118.48], Avg: -27.9351 (0.825)
Ep: 63, Reward: 165.4321, Test: 139.6277 [131.64], Avg: -27.3738 (0.823)
Ep: 64, Reward: 129.0172, Test: 49.9356 [101.55], Avg: -27.7468 (0.820)
Ep: 65, Reward: 101.0826, Test: 66.2490 [88.89], Avg: -27.6694 (0.818)
Ep: 66, Reward: 141.1831, Test: 82.3316 [110.68], Avg: -27.6795 (0.815)
Ep: 67, Reward: 106.2720, Test: 31.5935 [68.82], Avg: -27.8199 (0.813)
Ep: 68, Reward: 71.9989, Test: 19.5067 [66.84], Avg: -28.1026 (0.810)
Ep: 69, Reward: 98.0897, Test: 67.5939 [89.05], Avg: -28.0077 (0.808)
Ep: 70, Reward: 82.4615, Test: 76.4721 [98.33], Avg: -27.9210 (0.805)
Ep: 71, Reward: 107.8031, Test: 33.7269 [59.16], Avg: -27.8864 (0.803)
Ep: 72, Reward: 50.3609, Test: 43.3472 [89.62], Avg: -28.1383 (0.801)
Ep: 73, Reward: 93.4097, Test: 150.6880 [84.87], Avg: -26.8686 (0.798)
Ep: 74, Reward: 100.6786, Test: 1.4942 [31.33], Avg: -26.9081 (0.796)
Ep: 75, Reward: 104.7723, Test: 51.9837 [75.80], Avg: -26.8674 (0.793)
Ep: 76, Reward: 65.2001, Test: 46.1883 [108.32], Avg: -27.3253 (0.791)
Ep: 77, Reward: 138.0117, Test: 48.4293 [87.19], Avg: -27.4719 (0.789)
Ep: 78, Reward: 141.1874, Test: 167.0465 [157.39], Avg: -27.0019 (0.786)
Ep: 79, Reward: 216.7443, Test: 168.1846 [128.98], Avg: -26.1743 (0.784)
Ep: 80, Reward: 188.7452, Test: 252.7555 [171.29], Avg: -24.8454 (0.782)
Ep: 81, Reward: 300.3209, Test: 203.1518 [139.49], Avg: -23.7660 (0.779)
Ep: 82, Reward: 285.3937, Test: 333.2333 [264.15], Avg: -22.6472 (0.777)
Ep: 83, Reward: 317.5547, Test: 187.4705 [119.39], Avg: -21.5672 (0.775)
Ep: 84, Reward: 320.6839, Test: 216.0279 [159.16], Avg: -20.6444 (0.772)
Ep: 85, Reward: 367.6839, Test: 188.2725 [113.85], Avg: -19.5390 (0.770)
Ep: 86, Reward: 359.0607, Test: 254.8777 [125.36], Avg: -17.8258 (0.768)
Ep: 87, Reward: 300.5163, Test: 328.3042 [264.61], Avg: -16.8994 (0.765)
Ep: 88, Reward: 381.1174, Test: 252.1071 [117.98], Avg: -15.2025 (0.763)
Ep: 89, Reward: 385.9165, Test: 331.8033 [204.70], Avg: -13.6213 (0.761)
Ep: 90, Reward: 423.5624, Test: 384.2457 [175.95], Avg: -11.1827 (0.758)
Ep: 91, Reward: 443.0583, Test: 355.7385 [180.06], Avg: -9.1516 (0.756)
Ep: 92, Reward: 322.8525, Test: 290.1130 [226.52], Avg: -8.3694 (0.754)
Ep: 93, Reward: 377.9082, Test: 278.3130 [216.37], Avg: -7.6213 (0.752)
Ep: 94, Reward: 375.5254, Test: 346.1532 [159.62], Avg: -5.5776 (0.749)
Ep: 95, Reward: 330.7696, Test: 286.9976 [170.71], Avg: -4.3082 (0.747)
Ep: 96, Reward: 357.1818, Test: 306.1893 [107.89], Avg: -2.2195 (0.745)
Ep: 97, Reward: 274.6335, Test: 200.8177 [122.95], Avg: -1.4023 (0.743)
Ep: 98, Reward: 310.2770, Test: 174.6847 [100.61], Avg: -0.6399 (0.740)
Ep: 99, Reward: 315.7616, Test: 220.2071 [189.60], Avg: -0.3275 (0.738)
Ep: 100, Reward: 324.8314, Test: 294.5927 [166.00], Avg: 0.9489 (0.736)
Ep: 101, Reward: 235.1948, Test: 275.8089 [133.86], Avg: 2.3313 (0.734)
Ep: 102, Reward: 291.1600, Test: 218.0559 [116.62], Avg: 3.2935 (0.732)
Ep: 103, Reward: 202.9025, Test: 243.5845 [155.45], Avg: 4.1092 (0.729)
Ep: 104, Reward: 223.3205, Test: 237.7584 [204.67], Avg: 4.3852 (0.727)
Ep: 105, Reward: 294.5995, Test: 194.2864 [114.31], Avg: 5.0984 (0.725)
Ep: 106, Reward: 335.5629, Test: 167.8533 [112.76], Avg: 5.5656 (0.723)
Ep: 107, Reward: 259.5398, Test: 215.3253 [134.48], Avg: 6.2626 (0.721)
Ep: 108, Reward: 311.0080, Test: 261.7668 [192.90], Avg: 6.8370 (0.719)
Ep: 109, Reward: 268.6012, Test: 271.3059 [136.17], Avg: 8.0034 (0.716)
Ep: 110, Reward: 308.8239, Test: 187.3023 [145.99], Avg: 8.3035 (0.714)
Ep: 111, Reward: 254.5812, Test: 234.3957 [209.86], Avg: 8.4484 (0.712)
Ep: 112, Reward: 277.6933, Test: 283.0719 [103.85], Avg: 9.9597 (0.710)
Ep: 113, Reward: 240.4733, Test: 219.5407 [128.50], Avg: 10.6709 (0.708)
Ep: 114, Reward: 447.6388, Test: 323.5000 [259.44], Avg: 11.1352 (0.706)
Ep: 115, Reward: 453.4486, Test: 593.6554 [307.57], Avg: 13.5054 (0.704)
Ep: 116, Reward: 656.9138, Test: 490.4213 [271.34], Avg: 15.2625 (0.702)
Ep: 117, Reward: 569.8260, Test: 412.0452 [368.57], Avg: 15.5016 (0.699)
Ep: 118, Reward: 503.6347, Test: 460.5818 [368.01], Avg: 16.1492 (0.697)
Ep: 119, Reward: 528.0097, Test: 628.0470 [313.13], Avg: 18.6390 (0.695)
Ep: 120, Reward: 528.7632, Test: 495.4562 [320.07], Avg: 19.9344 (0.693)
Ep: 121, Reward: 577.1552, Test: 458.2372 [334.01], Avg: 20.7892 (0.691)
Ep: 122, Reward: 478.3948, Test: 440.1964 [293.48], Avg: 21.8131 (0.689)
Ep: 123, Reward: 475.1174, Test: 280.3873 [241.46], Avg: 21.9511 (0.687)
Ep: 124, Reward: 474.2717, Test: 255.3858 [196.75], Avg: 22.2446 (0.685)
Ep: 125, Reward: 398.4458, Test: 309.4032 [168.56], Avg: 23.1859 (0.683)
Ep: 126, Reward: 383.8793, Test: 358.1522 [229.94], Avg: 24.0129 (0.681)
Ep: 127, Reward: 499.9038, Test: 509.2034 [215.76], Avg: 26.1178 (0.679)
Ep: 128, Reward: 466.6640, Test: 391.3753 [300.57], Avg: 26.6193 (0.677)
Ep: 129, Reward: 390.5783, Test: 277.6987 [235.84], Avg: 26.7364 (0.675)
Ep: 130, Reward: 327.4896, Test: 289.7692 [233.72], Avg: 26.9602 (0.673)
Ep: 131, Reward: 377.7291, Test: 284.5528 [269.65], Avg: 26.8688 (0.671)
Ep: 132, Reward: 407.9997, Test: 295.0121 [327.92], Avg: 26.4193 (0.669)
Ep: 133, Reward: 339.4485, Test: 143.7461 [198.07], Avg: 25.8168 (0.667)
Ep: 134, Reward: 358.5474, Test: 332.8714 [283.23], Avg: 25.9933 (0.665)
Ep: 135, Reward: 416.1960, Test: 531.1426 [249.71], Avg: 27.8715 (0.663)
Ep: 136, Reward: 525.9255, Test: 378.9076 [329.24], Avg: 28.0306 (0.661)
Ep: 137, Reward: 466.9050, Test: 317.1753 [278.48], Avg: 28.1079 (0.659)
Ep: 138, Reward: 246.7239, Test: 192.5121 [187.29], Avg: 27.9432 (0.657)
Ep: 139, Reward: 156.0475, Test: 307.5871 [240.91], Avg: 28.2199 (0.655)
Ep: 140, Reward: 320.8464, Test: 347.5215 [275.81], Avg: 28.5284 (0.653)
Ep: 141, Reward: 353.8965, Test: 269.1091 [123.78], Avg: 29.3509 (0.651)
Ep: 142, Reward: 308.7407, Test: 234.2610 [146.27], Avg: 29.7610 (0.649)
Ep: 143, Reward: 365.1637, Test: 248.1272 [211.77], Avg: 29.8068 (0.647)
Ep: 144, Reward: 250.1206, Test: 124.2788 [110.97], Avg: 29.6930 (0.645)
Ep: 145, Reward: 230.4401, Test: 365.8127 [364.64], Avg: 29.4976 (0.643)
Ep: 146, Reward: 553.2953, Test: 535.9479 [225.86], Avg: 31.4064 (0.641)
Ep: 147, Reward: 408.6851, Test: 343.9350 [163.94], Avg: 32.4103 (0.639)
Ep: 148, Reward: 435.8096, Test: 395.3804 [220.51], Avg: 33.3664 (0.637)
Ep: 149, Reward: 428.5920, Test: 424.1103 [277.73], Avg: 34.1199 (0.635)
Ep: 150, Reward: 332.3300, Test: 345.5867 [264.80], Avg: 34.4289 (0.633)
Ep: 151, Reward: 422.4861, Test: 490.9655 [280.84], Avg: 35.5848 (0.631)
Ep: 152, Reward: 521.8910, Test: 491.0506 [311.85], Avg: 36.5235 (0.630)
Ep: 153, Reward: 576.7186, Test: 373.3870 [366.65], Avg: 36.3300 (0.628)
Ep: 154, Reward: 398.3397, Test: 604.3292 [188.87], Avg: 38.7760 (0.626)
Ep: 155, Reward: 426.1289, Test: 377.6225 [367.64], Avg: 38.5914 (0.624)
Ep: 156, Reward: 447.1587, Test: 277.6831 [275.29], Avg: 38.3608 (0.622)
Ep: 157, Reward: 296.3478, Test: 519.3363 [329.21], Avg: 39.3214 (0.620)
Ep: 158, Reward: 431.7350, Test: 342.4092 [253.65], Avg: 39.6323 (0.618)
Ep: 159, Reward: 319.0622, Test: 397.0809 [285.81], Avg: 40.0800 (0.616)
Ep: 160, Reward: 527.7085, Test: 290.4922 [253.06], Avg: 40.0636 (0.615)
Ep: 161, Reward: 318.2657, Test: 206.6482 [221.72], Avg: 39.7232 (0.613)
Ep: 162, Reward: 236.3824, Test: 317.4481 [217.01], Avg: 40.0957 (0.611)
Ep: 163, Reward: 269.8192, Test: 157.7082 [197.00], Avg: 39.6117 (0.609)
Ep: 164, Reward: 272.1453, Test: 281.5123 [106.78], Avg: 40.4306 (0.607)
Ep: 165, Reward: 286.2132, Test: 222.5781 [150.31], Avg: 40.6224 (0.605)
Ep: 166, Reward: 269.8999, Test: 364.1793 [180.36], Avg: 41.4799 (0.604)
Ep: 167, Reward: 294.9306, Test: 326.8074 [187.43], Avg: 42.0626 (0.602)
Ep: 168, Reward: 394.6579, Test: 470.1027 [120.50], Avg: 43.8824 (0.600)
Ep: 169, Reward: 545.0973, Test: 369.7284 [237.11], Avg: 44.4044 (0.598)
Ep: 170, Reward: 400.4787, Test: 260.8006 [209.92], Avg: 44.4423 (0.596)
Ep: 171, Reward: 330.6501, Test: 124.6334 [81.30], Avg: 44.4359 (0.595)
Ep: 172, Reward: 203.2546, Test: 234.1185 [83.80], Avg: 45.0479 (0.593)
Ep: 173, Reward: 174.3742, Test: 178.4106 [71.89], Avg: 45.4012 (0.591)
Ep: 174, Reward: 145.1240, Test: 216.7782 [118.36], Avg: 45.7041 (0.589)
Ep: 175, Reward: 178.5352, Test: 91.2059 [158.46], Avg: 45.0623 (0.588)
Ep: 176, Reward: 230.6260, Test: 257.2323 [88.94], Avg: 45.7585 (0.586)
Ep: 177, Reward: 274.0057, Test: 194.0873 [123.20], Avg: 45.8996 (0.584)
Ep: 178, Reward: 196.1183, Test: -104.3042 [17.99], Avg: 44.9600 (0.582)
Ep: 179, Reward: -7.9316, Test: 258.2551 [178.74], Avg: 45.1519 (0.581)
Ep: 180, Reward: 179.0241, Test: 215.5577 [138.14], Avg: 45.3302 (0.579)
Ep: 181, Reward: 226.6272, Test: 283.7498 [202.29], Avg: 45.5287 (0.577)
Ep: 182, Reward: 284.3391, Test: 235.6639 [207.95], Avg: 45.4313 (0.575)
Ep: 183, Reward: 274.0568, Test: 250.9919 [216.97], Avg: 45.3693 (0.574)
Ep: 184, Reward: 292.9469, Test: 205.0170 [215.90], Avg: 45.0653 (0.572)
Ep: 185, Reward: 275.3519, Test: 217.0178 [183.24], Avg: 45.0046 (0.570)
Ep: 186, Reward: 189.8783, Test: 138.2207 [133.38], Avg: 44.7898 (0.568)
Ep: 187, Reward: 158.4588, Test: 67.4355 [67.61], Avg: 44.5506 (0.567)
Ep: 188, Reward: 74.4628, Test: 147.6148 [169.24], Avg: 44.2004 (0.565)
Ep: 189, Reward: 149.7950, Test: 153.1790 [116.84], Avg: 44.1591 (0.563)
Ep: 190, Reward: 87.0436, Test: 21.5077 [62.94], Avg: 43.7109 (0.562)
Ep: 191, Reward: -1.0167, Test: -21.2820 [12.48], Avg: 43.3074 (0.560)
Ep: 192, Reward: -29.5390, Test: -30.6025 [38.19], Avg: 42.7266 (0.558)
Ep: 193, Reward: -31.8941, Test: -28.8690 [34.24], Avg: 42.1811 (0.557)
Ep: 194, Reward: -5.2508, Test: -22.7863 [14.99], Avg: 41.7710 (0.555)
Ep: 195, Reward: -18.0560, Test: -5.0565 [26.14], Avg: 41.3987 (0.553)
Ep: 196, Reward: 3.8796, Test: -3.1281 [17.25], Avg: 41.0851 (0.552)
Ep: 197, Reward: -2.7907, Test: -8.8206 [17.70], Avg: 40.7437 (0.550)
Ep: 198, Reward: -5.7908, Test: 15.5529 [40.91], Avg: 40.4115 (0.548)
Ep: 199, Reward: 13.2159, Test: -1.5747 [20.03], Avg: 40.1014 (0.547)
Ep: 200, Reward: 4.2588, Test: 31.3874 [50.12], Avg: 39.8087 (0.545)
Ep: 201, Reward: 1.7801, Test: 7.6556 [26.38], Avg: 39.5189 (0.543)
Ep: 202, Reward: 6.0668, Test: 2.5819 [25.72], Avg: 39.2103 (0.542)
Ep: 203, Reward: -8.4323, Test: 17.6569 [39.05], Avg: 38.9132 (0.540)
Ep: 204, Reward: 8.0841, Test: 15.1189 [23.46], Avg: 38.6827 (0.539)
Ep: 205, Reward: 34.4492, Test: -9.0356 [12.24], Avg: 38.3916 (0.537)
Ep: 206, Reward: 2.7467, Test: -12.9571 [8.53], Avg: 38.1024 (0.535)
Ep: 207, Reward: 3.4861, Test: 18.7208 [51.07], Avg: 37.7637 (0.534)
Ep: 208, Reward: -4.8317, Test: -7.3680 [17.89], Avg: 37.4622 (0.532)
Ep: 209, Reward: 19.5721, Test: -7.7230 [17.79], Avg: 37.1623 (0.530)
Ep: 210, Reward: 12.4704, Test: -10.3135 [22.17], Avg: 36.8322 (0.529)
Ep: 211, Reward: 6.8922, Test: 3.1396 [37.05], Avg: 36.4985 (0.527)
Ep: 212, Reward: 57.4156, Test: 2.1523 [16.75], Avg: 36.2586 (0.526)
Ep: 213, Reward: 3.3758, Test: 20.0694 [53.30], Avg: 35.9339 (0.524)
Ep: 214, Reward: 3.1454, Test: 58.0958 [70.98], Avg: 35.7068 (0.523)
Ep: 215, Reward: 23.0640, Test: 6.6577 [28.58], Avg: 35.4400 (0.521)
Ep: 216, Reward: 1.7946, Test: 8.2994 [36.51], Avg: 35.1467 (0.519)
Ep: 217, Reward: 13.1815, Test: 23.4622 [55.90], Avg: 34.8367 (0.518)
Ep: 218, Reward: 20.5205, Test: 18.8525 [63.74], Avg: 34.4727 (0.516)
Ep: 219, Reward: 49.1964, Test: 32.2408 [70.05], Avg: 34.1441 (0.515)
Ep: 220, Reward: 33.6449, Test: -3.0937 [13.72], Avg: 33.9135 (0.513)
Ep: 221, Reward: 51.4495, Test: 22.3530 [58.04], Avg: 33.6000 (0.512)
Ep: 222, Reward: 27.8978, Test: 30.6949 [42.81], Avg: 33.3950 (0.510)
Ep: 223, Reward: 32.1872, Test: 12.8415 [30.10], Avg: 33.1689 (0.509)
Ep: 224, Reward: 78.7499, Test: 83.2507 [92.90], Avg: 32.9786 (0.507)
Ep: 225, Reward: 80.3584, Test: 42.3347 [74.94], Avg: 32.6884 (0.506)
Ep: 226, Reward: 89.5570, Test: 18.9591 [21.96], Avg: 32.5312 (0.504)
Ep: 227, Reward: 55.5876, Test: 37.2266 [75.54], Avg: 32.2205 (0.503)
Ep: 228, Reward: -2.2892, Test: -1.4141 [29.17], Avg: 31.9462 (0.501)
Ep: 229, Reward: 8.6234, Test: 44.6252 [85.54], Avg: 31.6294 (0.500)
Ep: 230, Reward: 33.9409, Test: 29.1625 [55.03], Avg: 31.3805 (0.498)
Ep: 231, Reward: 12.4558, Test: 77.8641 [90.56], Avg: 31.1905 (0.497)
Ep: 232, Reward: 82.3371, Test: 67.7018 [75.32], Avg: 31.0240 (0.495)
Ep: 233, Reward: 97.1134, Test: 46.5674 [79.72], Avg: 30.7497 (0.494)
Ep: 234, Reward: 23.4043, Test: 16.8965 [38.26], Avg: 30.5280 (0.492)
Ep: 235, Reward: 45.6843, Test: 16.3866 [27.40], Avg: 30.3519 (0.491)
Ep: 236, Reward: 48.7176, Test: 38.4503 [75.85], Avg: 30.0661 (0.489)
Ep: 237, Reward: 42.3189, Test: 77.1764 [111.99], Avg: 29.7935 (0.488)
Ep: 238, Reward: 27.6705, Test: 12.7569 [41.38], Avg: 29.5491 (0.486)
Ep: 239, Reward: -2.3917, Test: 44.2777 [71.03], Avg: 29.3144 (0.485)
Ep: 240, Reward: 28.3519, Test: 11.1501 [47.02], Avg: 29.0440 (0.483)
Ep: 241, Reward: 17.8037, Test: -2.5497 [57.38], Avg: 28.6763 (0.482)
Ep: 242, Reward: 7.7505, Test: -27.7017 [38.85], Avg: 28.2844 (0.480)
Ep: 243, Reward: -15.2262, Test: 41.0686 [62.00], Avg: 28.0827 (0.479)
Ep: 244, Reward: 17.1966, Test: 5.7783 [18.25], Avg: 27.9172 (0.478)
Ep: 245, Reward: -5.7195, Test: -6.1164 [18.60], Avg: 27.7033 (0.476)
Ep: 246, Reward: 13.8370, Test: 3.7443 [20.99], Avg: 27.5213 (0.475)
Ep: 247, Reward: 7.4058, Test: 1.6561 [20.38], Avg: 27.3348 (0.473)
Ep: 248, Reward: -0.2418, Test: 8.5086 [50.47], Avg: 27.0565 (0.472)
Ep: 249, Reward: 27.6230, Test: 7.1362 [24.03], Avg: 26.8807 (0.470)
Ep: 250, Reward: 21.1522, Test: 31.2598 [57.67], Avg: 26.6684 (0.469)
Ep: 251, Reward: 22.3650, Test: -3.3828 [19.41], Avg: 26.4721 (0.468)
Ep: 252, Reward: 4.3655, Test: 3.3967 [16.85], Avg: 26.3143 (0.466)
Ep: 253, Reward: 4.8952, Test: -0.7904 [19.41], Avg: 26.1312 (0.465)
Ep: 254, Reward: 37.2664, Test: 2.5495 [32.33], Avg: 25.9119 (0.463)
Ep: 255, Reward: 47.7793, Test: 13.2160 [41.47], Avg: 25.7003 (0.462)
Ep: 256, Reward: 11.5276, Test: 16.7608 [41.39], Avg: 25.5045 (0.461)
Ep: 257, Reward: 37.0096, Test: 31.9270 [43.78], Avg: 25.3597 (0.459)
Ep: 258, Reward: 8.6547, Test: 12.9992 [33.98], Avg: 25.1808 (0.458)
Ep: 259, Reward: 10.5630, Test: 25.2653 [53.64], Avg: 24.9748 (0.456)
Ep: 260, Reward: 23.1794, Test: 6.4649 [31.56], Avg: 24.7829 (0.455)
Ep: 261, Reward: 22.5390, Test: 12.3275 [22.24], Avg: 24.6505 (0.454)
Ep: 262, Reward: 13.5150, Test: 19.1999 [53.10], Avg: 24.4279 (0.452)
Ep: 263, Reward: 14.3850, Test: 23.0598 [62.41], Avg: 24.1863 (0.451)
Ep: 264, Reward: 26.3641, Test: 12.0441 [22.72], Avg: 24.0547 (0.450)
Ep: 265, Reward: 5.6154, Test: 17.6390 [53.28], Avg: 23.8303 (0.448)
Ep: 266, Reward: 18.4038, Test: 42.6767 [71.97], Avg: 23.6313 (0.447)
Ep: 267, Reward: 36.1064, Test: -8.6202 [23.37], Avg: 23.4238 (0.446)
Ep: 268, Reward: 16.6024, Test: -8.6021 [15.56], Avg: 23.2469 (0.444)
Ep: 269, Reward: 12.7307, Test: 3.4246 [65.40], Avg: 22.9312 (0.443)
Ep: 270, Reward: -5.0753, Test: 8.5717 [58.06], Avg: 22.6640 (0.442)
Ep: 271, Reward: -3.3130, Test: 8.4535 [51.41], Avg: 22.4227 (0.440)
Ep: 272, Reward: -2.6862, Test: 20.9005 [72.84], Avg: 22.1504 (0.439)
Ep: 273, Reward: 12.4918, Test: -9.9954 [25.57], Avg: 21.9397 (0.438)
Ep: 274, Reward: 21.0002, Test: 5.9944 [43.02], Avg: 21.7253 (0.436)
Ep: 275, Reward: 3.5866, Test: -1.8320 [33.21], Avg: 21.5196 (0.435)
Ep: 276, Reward: 3.2339, Test: -3.6629 [31.47], Avg: 21.3151 (0.434)
Ep: 277, Reward: 8.8271, Test: 4.5608 [56.79], Avg: 21.0506 (0.432)
Ep: 278, Reward: -3.1468, Test: -47.8370 [30.01], Avg: 20.6961 (0.431)
Ep: 279, Reward: -34.4949, Test: -45.8671 [50.85], Avg: 20.2768 (0.430)
Ep: 280, Reward: -37.7869, Test: -27.7632 [16.11], Avg: 20.0485 (0.429)
Ep: 281, Reward: -16.1171, Test: -28.8304 [43.72], Avg: 19.7201 (0.427)
Ep: 282, Reward: -22.7515, Test: -34.7359 [15.37], Avg: 19.4734 (0.426)
Ep: 283, Reward: -17.6100, Test: -45.2925 [2.36], Avg: 19.2370 (0.425)
Ep: 284, Reward: -42.1498, Test: -25.4584 [19.03], Avg: 19.0134 (0.423)
Ep: 285, Reward: -38.7790, Test: 4.3461 [47.77], Avg: 18.7951 (0.422)
Ep: 286, Reward: -2.2927, Test: -3.1267 [24.33], Avg: 18.6339 (0.421)
Ep: 287, Reward: -5.9155, Test: -4.8984 [27.75], Avg: 18.4559 (0.420)
Ep: 288, Reward: -11.8904, Test: 14.0019 [50.17], Avg: 18.2668 (0.418)
Ep: 289, Reward: 6.8845, Test: 10.4915 [31.00], Avg: 18.1331 (0.417)
Ep: 290, Reward: 1.9597, Test: 4.4749 [42.09], Avg: 17.9416 (0.416)
Ep: 291, Reward: 8.7681, Test: 2.1164 [35.37], Avg: 17.7662 (0.415)
Ep: 292, Reward: 4.0473, Test: 8.5084 [32.04], Avg: 17.6253 (0.413)
Ep: 293, Reward: 3.8281, Test: -10.1635 [25.37], Avg: 17.4445 (0.412)
Ep: 294, Reward: -16.0631, Test: -11.8323 [44.03], Avg: 17.1960 (0.411)
Ep: 295, Reward: -26.7417, Test: -20.9199 [19.11], Avg: 17.0027 (0.410)
Ep: 296, Reward: -27.2370, Test: -7.2247 [27.71], Avg: 16.8278 (0.408)
Ep: 297, Reward: -24.0337, Test: -13.5015 [34.19], Avg: 16.6113 (0.407)
Ep: 298, Reward: -20.8959, Test: -14.4198 [24.98], Avg: 16.4240 (0.406)
Ep: 299, Reward: -24.2725, Test: -26.0571 [25.19], Avg: 16.1984 (0.405)
Ep: 300, Reward: -25.4583, Test: -26.4789 [33.15], Avg: 15.9465 (0.404)
Ep: 301, Reward: -26.7786, Test: -4.1283 [42.37], Avg: 15.7397 (0.402)
Ep: 302, Reward: -27.9530, Test: 16.7574 [59.47], Avg: 15.5468 (0.401)
Ep: 303, Reward: -16.7373, Test: -26.5384 [24.28], Avg: 15.3285 (0.400)
Ep: 304, Reward: -16.4883, Test: 0.7948 [41.13], Avg: 15.1460 (0.399)
Ep: 305, Reward: -25.7154, Test: -17.0041 [21.87], Avg: 14.9694 (0.398)
Ep: 306, Reward: -9.9316, Test: -1.8163 [24.98], Avg: 14.8334 (0.396)
Ep: 307, Reward: -14.8620, Test: 14.2648 [35.76], Avg: 14.7154 (0.395)
Ep: 308, Reward: -15.0501, Test: 2.1713 [38.73], Avg: 14.5495 (0.394)
Ep: 309, Reward: -4.2793, Test: -15.4961 [15.59], Avg: 14.4023 (0.393)
Ep: 310, Reward: 2.6916, Test: -11.9927 [27.60], Avg: 14.2287 (0.392)
Ep: 311, Reward: -6.7673, Test: -15.6520 [20.15], Avg: 14.0683 (0.390)
Ep: 312, Reward: -20.4130, Test: 10.3891 [46.27], Avg: 13.9087 (0.389)
Ep: 313, Reward: -7.3293, Test: 4.6905 [46.00], Avg: 13.7329 (0.388)
Ep: 314, Reward: 4.1048, Test: -22.7279 [24.25], Avg: 13.5401 (0.387)
Ep: 315, Reward: -12.1781, Test: -21.2127 [15.09], Avg: 13.3824 (0.386)
Ep: 316, Reward: -7.3134, Test: -19.7044 [22.10], Avg: 13.2083 (0.385)
Ep: 317, Reward: -19.4554, Test: -4.1085 [31.90], Avg: 13.0535 (0.383)
Ep: 318, Reward: -8.2648, Test: -26.7262 [9.78], Avg: 12.8982 (0.382)
Ep: 319, Reward: -18.6113, Test: -21.6390 [18.56], Avg: 12.7323 (0.381)
Ep: 320, Reward: -7.3840, Test: -29.8923 [15.94], Avg: 12.5498 (0.380)
Ep: 321, Reward: -24.7547, Test: -15.1356 [18.00], Avg: 12.4080 (0.379)
Ep: 322, Reward: -23.9163, Test: -23.9566 [17.88], Avg: 12.2400 (0.378)
Ep: 323, Reward: -24.5389, Test: -18.0635 [17.16], Avg: 12.0935 (0.377)
Ep: 324, Reward: -7.7976, Test: 1.1763 [35.82], Avg: 11.9497 (0.376)
Ep: 325, Reward: -21.5513, Test: -9.5549 [21.48], Avg: 11.8178 (0.374)
Ep: 326, Reward: -12.9519, Test: -19.8491 [19.57], Avg: 11.6612 (0.373)
Ep: 327, Reward: -11.1182, Test: -13.3461 [23.47], Avg: 11.5134 (0.372)
Ep: 328, Reward: -1.7889, Test: 2.8316 [42.78], Avg: 11.3570 (0.371)
Ep: 329, Reward: -8.9540, Test: 5.3367 [44.61], Avg: 11.2035 (0.370)
Ep: 330, Reward: -8.2613, Test: 12.0253 [37.29], Avg: 11.0933 (0.369)
Ep: 331, Reward: 4.9115, Test: -4.5051 [23.43], Avg: 10.9758 (0.368)
Ep: 332, Reward: -6.7832, Test: -17.2975 [16.38], Avg: 10.8417 (0.367)
Ep: 333, Reward: -7.9920, Test: -15.9309 [8.92], Avg: 10.7348 (0.365)
Ep: 334, Reward: -14.1377, Test: -7.4822 [14.77], Avg: 10.6364 (0.364)
Ep: 335, Reward: -2.6640, Test: 12.7193 [26.19], Avg: 10.5646 (0.363)
Ep: 336, Reward: 13.5801, Test: -0.4379 [17.63], Avg: 10.4796 (0.362)
Ep: 337, Reward: 8.1307, Test: 22.6651 [50.40], Avg: 10.3666 (0.361)
Ep: 338, Reward: 26.8720, Test: 14.2137 [39.75], Avg: 10.2607 (0.360)
Ep: 339, Reward: 15.4389, Test: 30.7324 [40.78], Avg: 10.2009 (0.359)
Ep: 340, Reward: 19.6566, Test: 11.0947 [40.20], Avg: 10.0857 (0.358)
Ep: 341, Reward: 14.5777, Test: -6.4123 [18.01], Avg: 9.9848 (0.357)
Ep: 342, Reward: 14.8288, Test: 17.9866 [37.72], Avg: 9.8981 (0.356)
Ep: 343, Reward: 10.5726, Test: 8.1039 [47.86], Avg: 9.7538 (0.355)
Ep: 344, Reward: 3.5926, Test: -2.3828 [32.52], Avg: 9.6243 (0.354)
Ep: 345, Reward: 7.2045, Test: 5.8784 [21.76], Avg: 9.5506 (0.353)
Ep: 346, Reward: 15.2349, Test: 39.0821 [49.07], Avg: 9.4943 (0.351)
Ep: 347, Reward: 37.5990, Test: 10.0684 [22.36], Avg: 9.4317 (0.350)
Ep: 348, Reward: 12.4071, Test: 22.6674 [43.90], Avg: 9.3438 (0.349)
Ep: 349, Reward: 25.0038, Test: 11.0526 [22.63], Avg: 9.2840 (0.348)
Ep: 350, Reward: 4.0855, Test: 7.8365 [31.01], Avg: 9.1916 (0.347)
Ep: 351, Reward: 42.0439, Test: 34.6261 [45.83], Avg: 9.1336 (0.346)
Ep: 352, Reward: 11.1077, Test: -8.1549 [19.78], Avg: 9.0286 (0.345)
Ep: 353, Reward: 19.1537, Test: -5.4828 [24.74], Avg: 8.9177 (0.344)
Ep: 354, Reward: 14.7783, Test: 12.0544 [63.49], Avg: 8.7477 (0.343)
Ep: 355, Reward: 10.1624, Test: -0.9215 [32.92], Avg: 8.6281 (0.342)
Ep: 356, Reward: 14.6607, Test: 5.6681 [24.99], Avg: 8.5498 (0.341)
Ep: 357, Reward: 1.5708, Test: 4.7784 [23.49], Avg: 8.4736 (0.340)
Ep: 358, Reward: 19.0195, Test: 17.5915 [33.95], Avg: 8.4045 (0.339)
Ep: 359, Reward: 8.8710, Test: 34.1870 [51.88], Avg: 8.3320 (0.338)
Ep: 360, Reward: 12.0073, Test: -1.2920 [29.30], Avg: 8.2242 (0.337)
Ep: 361, Reward: -0.1359, Test: -11.0741 [18.76], Avg: 8.1191 (0.336)
Ep: 362, Reward: 15.8078, Test: -1.9652 [20.87], Avg: 8.0338 (0.335)
Ep: 363, Reward: 5.5792, Test: 17.1699 [40.99], Avg: 7.9463 (0.334)
Ep: 364, Reward: 19.6296, Test: 11.0671 [38.91], Avg: 7.8482 (0.333)
Ep: 365, Reward: 3.2961, Test: 1.9688 [21.82], Avg: 7.7725 (0.332)
Ep: 366, Reward: 17.6468, Test: -3.1880 [16.00], Avg: 7.6991 (0.331)
Ep: 367, Reward: 4.6463, Test: 30.9811 [44.52], Avg: 7.6414 (0.330)
Ep: 368, Reward: 29.2779, Test: -4.0583 [11.33], Avg: 7.5789 (0.329)
Ep: 369, Reward: 28.8777, Test: 14.0934 [29.57], Avg: 7.5166 (0.328)
Ep: 370, Reward: 9.2890, Test: 0.1703 [12.92], Avg: 7.4620 (0.327)
Ep: 371, Reward: 11.5645, Test: 19.1651 [28.44], Avg: 7.4170 (0.326)
Ep: 372, Reward: 32.5110, Test: 36.4092 [50.06], Avg: 7.3605 (0.325)
Ep: 373, Reward: 37.0040, Test: 3.8161 [49.13], Avg: 7.2197 (0.324)
Ep: 374, Reward: 46.0455, Test: 19.9664 [24.71], Avg: 7.1878 (0.323)
Ep: 375, Reward: 39.3274, Test: 35.5245 [53.69], Avg: 7.1204 (0.322)
