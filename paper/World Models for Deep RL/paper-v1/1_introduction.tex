\section{INTRODUCTION}

Deep Reinforcement Learning is a growing field where neural networks are employed to learn a general policy for selecting actions in a given environment. State of the art reinforcement learning algorithms such as Q-Learning have evolved to allow neural networks to learn effective policies for simple control tasks such as balancing a cartpole or swinging a pendulum. In these cases, the agent often has access to the true dynamics of the environment as a low dimensional observation and is therefore able to learn optimal policies using a simple feed forward neural network. However, tasks such as learning to control a simulated car and then navigate a track given only raw images are much more difficult to learn with standard Deep Q-Learning algorithms. Furthermore, as dynamic information such as velocity and acceleration cannot be inferred from a single image, requiring the agent to either train an RNN to store the state history or receive input of stacked image frames to translate to optimal actions to select. This increases the computational load on deep RL where it has to learn a representation of the temporal structure in addition to learning the optimal policy. Therefore, this leads to the motivation to decouple the reinforcement learning task into (1) environment simplification, and (2) optimal action selection.

The task of learning an optimal policy from high dimensional images has many practical applications in real-life situations where the true dynamics are often unknown. Autonomous driving using computer vision control systems is a growing area of study where sensory data is often in image form \cite{4.0}. For these tasks, the ability to learn a stable control policy for safely driving a vehicle is a challenging optimization problem for deep reinforcement learning agents using large convolutional layers to process these images. Robotics is another application where deep reinforcement learning agents have been applied to train a robot arm to manipulate objects on a table from visual observations \cite{5.0}. This often requires thousands of training episodes to adequately learn the task, where the agent's representation of the environment may not even be transferable to other similar tasks. Since the latent features of a state space are generally constant, there is potential to improve training efficiency with offline learning of a mapping from high-dimensional images to a simplified environment state space, then using this model to compress the visual observation into a smaller dimensional input to train a policy via reinforcement learning. By modeling the environment separately, the learned model can be transferred to other agents learning different tasks from the same environment observations.

Previous approaches to addressing this problem have use a convolutional neural network to compress the state to latent vector and then have a separate RNN to learn the temporal history, however, the representation of the dynamics is coupled with the policy learned. As a result, using the trained convolutional network and RNN to train a new policy from scratch would introduce irrelevant features for a policy to learn from without retraining the CNN and RNN when learning. Ha and Schmidhuber \cite{1.0.0} recently introduced the ``World Models'' architecture which decouples of the environment representation from policy learning. This involved a visual module that maps high dimensional images to a lower dimensional code, a memory module that modeled temporal dynamics of the environment, and a simple decision-making module that determines the action an agent should take based on the compressed visual and temporal representations. Using Covariance-Matrix Adaptation evolutionary strategy (CMA) \cite{1.0.3} to train the decision-making module, this system achieved a new state of the art performance for a challenging simulated car racing environment with purely visual observations. However, CMA requires many iterations to search for a policy and is also only effective when the parameter space is small. Using RL algorithms to train using the world model is an unexplored approach for efficiently learning a policy for a model with more parameters. In this paper, we incorporate the World Model architecture to simplify the dimensionality of the image observations received from the environment to allow a reinforcement learning agent to focus on maximizing reward using the relevant features of the environment observations. We then evaluate the performance of such agents relative to the agent in \cite{1.0.0} on the same car racing environment. Our approach achieves state of the art performance on this environment without requiring any domain-specific modifications to the environment outputs.

This report will be structured as follows: Section \ref{background} provides a background to the field of reinforcement learning and the car racing environment as well as previous approaches to solve it; Section \ref{approach} outlines the architecture and approach for training the reinforcement agents using the World Model; and Section \ref{experiments} provides an experimental evaluation of the agents trained.


% The challenge here is that in environments with high dimensional non-markov state inputs (like pixels) the RL algorithm either needs to train an RNN to store the state history or receive input of stacked image frames. This increases the computational load on deep RL where it has to learn a representation of the temporal structure in addition to learning the optimal policy. Even when a representation is learned, this isn’t transferrable to other policies learning with the same environment dynamics.

% Why is it interesting and important? It is beneficial to learn a latent model of the high dimensional state space separate from training a policy because the environment dynamics are generally constant, so it is redundant for different policies trained from scratch on the same environment to have to learn this representation each time. Offline learning of a simplified environment state space is particularly applicable in robotics or self driving cars where the latent features of a state space are generally constant, but different tasks will require different policies, hence the learning of the state representation can be separated from learning the policy.

% Why is it hard? The naive approach is to use a convolutional neural network to compress the state to latent vector and then have a separate RNN to learn the temporal history however the representation of the dynamics is coupled with the policy learned, so using the trained convolutional network and RNN to train a new policy from scratch would introduce irrelevant features for a policy to learn from without retraining the CNN and RNN when learning (which would essentially be training from scratch)

% Why hasn’t it been solved before? Previous approaches to addressing this problem have used the method described in 3., however a recent paper “World Models” introduced this decoupling of the environment representation to policy where the policy is trained by CMA evolution. However, CMA requires many iterations to search for a policy and is also only effective when the parameter space is small. Using RL algorithms to train using the world model is an unexplored approach for efficiently learning a policy for a model with more parameters.

% What are the key components? Key components include using Deep RL algorithms such as PPO and DDPG in place of CMA to speed up training. This is evaluated on the CarRacing-v0 environment on OpenAI gym and compared to other papers solving that environment, my approach achieves state of the art performance (100 episode average reward of 900+ for both DDPG and PPO) whereas other attempts have achieved a maximum of 600 and others also require discretization of the continuous action space using domain knowledge. My approach doesn’t require any modifications to the environment outputs.