\begin{abstract}

Deep reinforcement learning (RL) has been used recently to solve control tasks with image inputs. Model-free RL algorithms are capable of learning a robust policy, but typically require many samples to learn an optimal policy. On the other hand, model-based methods improve sample efficiency, though there are challenges in applying them to high-dimensional observation spaces. The recent model-based approach for image domains known as ``World Models'' attempts to address the issue of sample complexity. It uses a variational auto-encoder and recurrent neural network to transform sequences of image observations to a latent feature vector and optimizes a simple controller network using an evolutionary algorithm. However, evolutionary algorithms often struggle to optimize policies with many parameters due to the exponential increase in the parameter search space. In this paper, we provide a general framework for incorporating the World Model architecture to train policies with deep RL, where the size of the network representing the policy is no longer constrained. We evaluate the efficiency of training RL agents using the World Model to compress high-dimensional image observations to low-dimensional feature representations. Results show that the World Model achieves more stable improvements in average reward as well as significantly decreasing training time. However, it was found that the World Model may not always learn a feature representation that is optimal for learning an optimal policy and hence may require fine tuning to the policy being learned. 
	
\end{abstract}