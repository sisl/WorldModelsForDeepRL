\section{CONCLUSIONS AND FUTURE WORK}

In this paper, we investigated the improvement in the efficiency of training reinforcement learning agents using the World Models architecture to compress high dimensional image observations to low dimensional feature representations. We proposed a general procedure for integrating World Models within any reinforcement learning task, allowing the environment representation to be learned offline, independently from the training of the policy. We also show that the resulting approach generalizes to different RL algorithms learning within the same environment. However, it was found that World Model may not encode the full scope of variation in the image states of the environment, leading to convergence to suboptimal policies in some environments.

For future work, the pre-trained World Model can be further optimized during policy optimization for fine-tuning of the latent features that are most important for action selection. Another direction is to improve the accuracy of the World Model with attention mechanisms. Overall, we found that the World Model architecture has potential to improve stability and speed of training RL policies in complex environments, in addition to generalizing to different algorithms.
