\section{EXPERIMENTAL SETUP}\label{experiments}

To evaluate the effectiveness of the World Model, we trained each RL algorithm for $500000$ steps on each of the three environments (except DDQN on \texttt{CarRacing-v0} due to continuous state space) with and without the World Model. The World Model was trained over two iterations for each component with $1000$ sampled trajectories added to the data set at each iteration, where the controller was trained for $500$ generations using CMA-ES. The training plots in \cref{fig:res} show the variability in the effect of the World Model on training efficiency. For most RL algorithms, training with the World Model outperformed the controller trained using CMA-ES in terms of maximum average reward.

The results for the \texttt{CarRacing-v0} environment in \cref{fig:res} (left) show that agents trained with the World Model appeared to achieve the same overall performance as agents trained using a convolutional network with stacked images. Although training PPO and DDPG without the World Model were able to achieve faster improvement in average reward, the agents trained using the World Model were able to maintain this performance with less variation in rewards at each training iteration. The time required to train agents with the World Model was significantly less, taking approximately 7 hours in contrast to the 11 hours required to train a new convolutional network for each agent.

The \texttt{take\_cover} environment rewards shown in \cref{fig:res} (middle) exhibit the most variation between agents using the Model and those without. It is clear that only the agents trained with the World Model were able to improve their average score while the agents training a convolutional network collapsed to a degenarate policy. Investigating playbacks of the trained agents revealed that policies converged to only move left. \Citeauthor{ha2018recurrent} referred to this being the result of the credit assignment problem where backpropagation of gradients through a large neural network with many layers and parameters can hinder convergence~\cite{ha2018recurrent}. However, the World Model enables the agent to use a shallower network for policy learning.

In the \texttt{defend_the_line} environment, training with the world model achieved a constant but stable reward compared to the agents using convolutional networks as seen in \cref{fig:res} (right). However, an outlying observation of SAC and DDQN converging to a much higher average reward when trained without a World Model suggests that the World Model may not always capture the optimal variational features for optimization of the policy for fine variations in the image states. To address this issue, the pre-trained World Model could be further trained along with the policy to allow for finer optimization at the image processing level.

As the plots show, the World Model has many benefits for deep RL. We demonstrated that a single pre-trained World Model can generalize the environment representation for use with different agents learning in the same environment, which can be applied to various domains with a stationary state distribution. It reduces the number of parameters that the RL agent needs to update, which reduces the time taken for each update, equivalently leading to faster training. Along with saving time, the World Model reduces the memory requirement of off-policy agents using a replay buffer where high dimensional states can be directly mapped to a latent state with the World Model before being stored.
