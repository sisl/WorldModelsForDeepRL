\section{INTRODUCTION}

Recent advances in computer vision with neural networks have enabled many real-world tasks in robotics~\cite{ebert2018visual, pinto2017asymmetric, levine2016end, peters2006policy}, autonomous driving~\cite{zhu2017target, wang2018deep}, and navigation~\cite{li2018reinforcement} to be learned with Deep Reinforcement Learning (RL) from high-dimensional images instead of low-dimensional sensor measurements. However, because RL agents trained in these domains receive a single image at a time, they are unable to infer temporal information such as velocity. As a result, RL agents often have to rely on recurrent neural networks (RNNs) or stacking sequences of recent images \cite{mnih2015human} to learn a feature representation of the underlying spatial and temporal dynamics. Both of these methods require a significant amount of computation for an RL agent to learn compared to simpler low-dimensional state spaces that can be learned with feed-forward networks. Hence, identifying a method to efficiently reduce the dimensionality of image inputs is an important challenge in improving the efficiency of Deep RL in image-based domains.

Model-free algorithms such as Deep Q-Networks (DQN) have achieved human-level performance in the image-based Atari2600 arcade game suite \cite{mnih2013playing, mnih2015human}. Deep Deterministic Policy Gradients (DDPG) \cite{lillicrap2015continuous}, Soft Actor Critic (SAC) \cite{haarnoja2018soft}, and Proximal Policy Optimization (PPO) \cite{schulman2017proximal} are state-of-the-art model-free methods for continuous control tasks, a domain in which most robotics and autonomous control applications operate. However, these algorithms reduce the dimensionality of image inputs with convolutional neural networks (CNNs) learned as a part of the policy to map images to actions, which requires many samples to sufficiently represent relevant features of the environment dynamics, leading to a high sample complexity \cite{buckman2018sample}. In contrast, model-based approaches learn an explicit model of the environment dynamics and have been used for model-predictive control (MPC) in multiple-joint control \cite{nagabandi2018neural, feinberg2018model, kurutach2018model, clavera2018model}, autonomous aircrafts \cite{abbeel2010autonomous}, and autonomous driving \cite{williams2017information}. However, these methods have primarily focused on low-dimensional state spaces. Since the applicability of RL in robotic and vehicle domains is heavily constrained by sample efficiency \cite{williams2017information, pinto2017asymmetric, danielczuk2019mechanical}, it is of great interest to improve their sample efficiency of model-free RL methods in these complex image domains.

Inspired by the successful applications in image generation \cite{pu2016variational,yan2016attribute2image}, auto-encoders have been used to model environments with image observations~\cite{hirose2019deep}. \citeauthor{kaiser2019model} used a VAE to learn an environment model for Atari games and applied PPO in the learned model, which significantly improved sample efficiency \cite{kaiser2019model}. The learned model can also be used for state compression, which reduces the learning load on the policy side. \citeauthor{wahlstrom2015pixels} used an auto-encoder to map visual inputs to low-dimensional features and applied MPC in the feature space \cite{wahlstrom2015pixels}. \citeauthor{corneil2018efficient} trained a VAE-based environment model encoding the observation history to a belief vector over discrete states, where the policy was represented using tabular Q learning \cite{corneil2018efficient}. The compressed latent space from a VAE could also be combined with state-of-the-art deep RL approaches. \citeauthor{igl2018deep} used the environment model as a particle filter, whose output is then encoded to serve as the state space for the A2C~\cite{openaibaselines} learner \cite{igl2018deep}. However, besides the complex structure of the VAE-based particle filter, a RNN is still needed on the policy side to encode the belief particles to the state vector.

\citeauthor{ha2018recurrent} introduced the ``World Models'' architecture, which decouples the environment representation task from the policy learning task~\cite{ha2018recurrent}. This architecture incorporates a visual compressor modeled by a Variational Auto-Encoder (VAE) that maps high dimensional images to a lower dimensional latent vector. Then, a memory component modeled by an RNN is trained to predict the temporal dynamics of the environment from an input action and latent state. A simple decision-making module determines the action an agent should take based on the compressed spatial and temporal representations. Using Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES)~\cite{hansen2016cma} to train the decision-making module, this system achieved state-of-the-art performance for a challenging simulated car racing environment with purely visual observations. However, a major limitation of CMA-ES is its limited scalability to networks with many parameters due to the exponential increase in search space with each additional parameter. On the other hand, training a model-free RL agent on the simplified state representation from the World Model allows for efficiently learning an optimal policy network of any size where the feature representation of the high dimensional states is already handled by the World Model.

In this paper, we incorporate the World Model architecture to reduce the dimensionality of image observations for training various model-free RL algorithms. We then evaluate the performance of agents trained with and without a World Model on image-based environments on the OpenAI and ViZDoom RL platform. We show that decoupling the environment representation task from the policy learning task significantly improves the sample efficiency training time of model-free RL algorithms.
