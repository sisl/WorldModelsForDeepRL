\section{CONCLUSION}

In this paper, we applied the recent World Models architecture to the field of deep reinforcement learning to simplify a high dimensional image state into a compressed feature vector for a reinforcement learning agent to learn an optimal policy. We focused on training two algorithms, DDPG and PPO, on OpenAI Gym's \texttt{CarRacing-v0} environment. It was found that the World Model significantly improved the stability of the PPO algorithm, allowing it to achieve an average score of 812 using the World Model compared to 153 without it. The DDPG algorithm achieved an average score of 842 with the World Model with a standard deviation of less than half of that without the World Model. Overall, our approach achieved the highest average score for the DDPG and PPO algorithm compared to previous approaches without having to simplify the continuous action space.

Future work could focus on addressing the credit assignment problem prevalent in reinforcement learning by incorporating generalized exploration schemes or adjusting the hyperparameters based on approaches for solving similarly complex environments. Another direction is to improve the accuracy of the World Model with attention mechanisms, or allowing the World Model to be optimized during policy optimization. Overall, we found that the World Model architecture has potential to improve stability and speed up the training of reinforcement learning agents in complex high dimensional environments, along with reducing the required size of the policy.