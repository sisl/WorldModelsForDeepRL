\begin{abstract}

We consider an application of the recently successful `World Model' architecture using generative neural networks in reinforcement learning to abstract high dimensional image observations into compressed spatial and temporal vector representation. We propose to incorporate the World Model with a deep reinforcement learning agent to learn an action-selection policy from the high-level features of the environment represented by the World Model. We tested two policy gradient actor-critic learning algorithms, Deep Deterministic Policy Gradients and Proximal Policy Optimization, trained using the World Model on OpenAI Gym's CarRacing-v0. We found that these agents were able to achieve scores at the level of the current state of the art evolutionary algorithms for training an agent. It was also observed that the simplified state representation of the World Model improved the speed and stability of the training process as well as enabling simpler neural network architectures than equivalent agents trained from raw image states.
	
\end{abstract}